---
layout: page
title: Wget
permalink: /workshops/wget/
---
## Wget

Wget is a free command line utility for non-interactive downloads of files from the web to retrieve online material. It supports HTTP, HTTPS, and FTP protocols, as well as retrieval through HTTP procies. Wget can also follow links in HTML, XHTML, and CSS pages, to create local versions of remote websites, fully recreating the directory structure of the original site. This means researchers can use wget to gather copies of online digital content for their research projects.

As you can imagine, wget can be quite useful if you are interested in building an archive of online data from websites, which could include documents, videos, images, and audio files. However, while wget can retrieve almost anything online it is important to read the end-user license agreements (EULAs) of websites you download from and know what falls under fair use when you download copies of files. In the documentation provided in this workshop, we only cover how you can download online digital files from public websites, such as Government Archives. Downloading data from social media accounts, digital game sites, and other login platform-based services is not covered in this workshop. If you are in doubt about whether your research project meets ethics requirements and fair use of copyrighted works you can consult your instituitonal research office and scholarly communications officer. If you do not have access to such resources take a look at our Resources page for more information.